{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as p\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count    Dtype \n",
      "---  ------      --------------    ----- \n",
      " 0   target      1600000 non-null  int64 \n",
      " 1   ids         1600000 non-null  int64 \n",
      " 2   date        1600000 non-null  object\n",
      " 3   flag        1600000 non-null  object\n",
      " 4   user        1600000 non-null  object\n",
      " 5   text        1600000 non-null  object\n",
      " 6   clean_text  1600000 non-null  object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 85.4+ MB\n"
     ]
    }
   ],
   "source": [
    "%store -r data\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600000\n",
      "1600000\n"
     ]
    }
   ],
   "source": [
    "X = data['clean_text'].values\n",
    "y = data['target'].values\n",
    "print(len(X))\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1584000,) (1584000,) (16000,) (16000,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.01, random_state=0)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_most_and_least_informative_features(vectorizer, clf, n=10):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[-n:])\n",
    "    print(\"Top \", n, \" most and least informative features\")\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print(\"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, y_train, X_test, y_test, vectorizer, clf):\n",
    "    \n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    print(\"X_train_vec shape - \", X_train_vec.shape, \"\\n\")\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    print(\"X_test_vec shape - \", X_test_vec.shape, \"\\n\")\n",
    "    \n",
    "    y_pred = clf.fit(X_train_vec, y_train).predict(X_test_vec)\n",
    "    show_most_and_least_informative_features(vectorizer, clf, n=20)\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=[0,4])\n",
    "    print()\n",
    "    print(\"Confusion matrix\\n\", cm, \"\\n\")\n",
    "\n",
    "    print(\"Classification report\\n\", classification_report(y_test, y_pred, target_names=['0','4'], digits=4))\n",
    "    \n",
    "    return y_pred, vectorizer, clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_vec shape -  (1584000, 243575) \n",
      "\n",
      "X_test_vec shape -  (16000, 243575) \n",
      "\n",
      "Top  20  most and least informative features\n",
      "\t-16.0232\taabot          \t\t-5.2207\tat             \n",
      "\t-16.0232\taabout         \t\t-5.1276\tbe             \n",
      "\t-16.0232\taabt           \t\t-4.9863\thave           \n",
      "\t-16.0232\taacattyisamazing\t\t-4.9837\tjust           \n",
      "\t-16.0232\taacchhoo       \t\t-4.9760\tgood           \n",
      "\t-16.0232\taacck          \t\t-4.9482\twith           \n",
      "\t-16.0232\taacckk         \t\t-4.9422\tso             \n",
      "\t-16.0232\taach           \t\t-4.8551\tme             \n",
      "\t-16.0232\taachar         \t\t-4.7055\tthat           \n",
      "\t-16.0232\taachee         \t\t-4.6925\ton             \n",
      "\t-16.0232\taachhoo        \t\t-4.6137\tof             \n",
      "\t-16.0232\taacount        \t\t-4.5088\tin             \n",
      "\t-16.0232\taacs           \t\t-4.4348\tis             \n",
      "\t-16.0232\taadaamm        \t\t-4.3605\tfor            \n",
      "\t-16.0232\taadam          \t\t-4.2897\tmy             \n",
      "\t-16.0232\taaden          \t\t-4.1297\tit             \n",
      "\t-16.0232\taadha          \t\t-4.1171\tand            \n",
      "\t-16.0232\taadn           \t\t-3.8365\tyou            \n",
      "\t-16.0232\taaeeaa         \t\t-3.5940\tto             \n",
      "\t-16.0232\taaeew          \t\t-3.5420\tthe            \n",
      "\n",
      "Confusion matrix\n",
      " [[6367 1546]\n",
      " [1889 6198]] \n",
      "\n",
      "Classification report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7712    0.8046    0.7876      7913\n",
      "           4     0.8004    0.7664    0.7830      8087\n",
      "\n",
      "    accuracy                         0.7853     16000\n",
      "   macro avg     0.7858    0.7855    0.7853     16000\n",
      "weighted avg     0.7859    0.7853    0.7853     16000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MNB unigram baseline model\n",
    "y_pred_mbn_1, vectorizer_mnb_1, clf_mnb_1 = model(X_train, y_train, X_test, y_test, CountVectorizer(encoding='latin-1'), MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:   49.7s\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed: 10.9min\n",
      "[Parallel(n_jobs=-1)]: Done 144 out of 144 | elapsed: 14.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set:\n",
      "[('tf', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='latin-1',\n",
      "                input='content', lowercase=True, max_df=0.2, max_features=None,\n",
      "                min_df=5, ngram_range=(1, 1), preprocessor=None,\n",
      "                stop_words=None, strip_accents=None,\n",
      "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None,\n",
      "                vocabulary=None)), ('nb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]\n"
     ]
    }
   ],
   "source": [
    "# MNB unigram tf tuning\n",
    "pipeline = Pipeline([('tf', CountVectorizer(encoding='latin-1')),('nb', MultinomialNB())])\n",
    "parameters = {\n",
    "    'tf__max_df': (0.01, 0.1, 0.2, 1.0),\n",
    "    'tf__min_df': (5, 10, 20, 1),\n",
    "    'nb__alpha': (1e-2, 1e-3, 1.0)\n",
    "}\n",
    "\n",
    "grid_search_tune = GridSearchCV(pipeline, parameters, cv=3, n_jobs=-1, verbose=3)\n",
    "grid_search_tune.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters set:\")\n",
    "print(grid_search_tune.best_estimator_.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_vec shape -  (1584000, 49535) \n",
      "\n",
      "X_test_vec shape -  (16000, 49535) \n",
      "\n",
      "Top  20  most and least informative features\n",
      "\t-15.9242\taaden          \t\t-5.1506\tday            \n",
      "\t-15.9242\taaghh          \t\t-5.1428\tbut            \n",
      "\t-15.9242\taarggh         \t\t-5.1217\tat             \n",
      "\t-15.9242\taargghh        \t\t-5.0287\tbe             \n",
      "\t-15.9242\taarrgg         \t\t-4.8873\thave           \n",
      "\t-15.9242\taarrghh        \t\t-4.8847\tjust           \n",
      "\t-15.9242\tabre           \t\t-4.8771\tgood           \n",
      "\t-15.9242\tabscess        \t\t-4.8492\twith           \n",
      "\t-15.9242\tabuser         \t\t-4.8433\tso             \n",
      "\t-15.9242\tabuses         \t\t-4.7562\tme             \n",
      "\t-15.9242\tacces          \t\t-4.6066\tthat           \n",
      "\t-15.9242\taccuweather    \t\t-4.5936\ton             \n",
      "\t-15.9242\tacidic         \t\t-4.5148\tof             \n",
      "\t-15.9242\tadamisarockstar\t\t-4.4099\tin             \n",
      "\t-15.9242\taddictedto     \t\t-4.3359\tis             \n",
      "\t-15.9242\tafriad         \t\t-4.2616\tfor            \n",
      "\t-15.9242\tafridi         \t\t-4.1907\tmy             \n",
      "\t-15.9242\tahar           \t\t-4.0308\tit             \n",
      "\t-15.9242\talergies       \t\t-4.0182\tand            \n",
      "\t-15.9242\tallergie       \t\t-3.7376\tyou            \n",
      "\n",
      "Confusion matrix\n",
      " [[6305 1608]\n",
      " [1844 6243]] \n",
      "\n",
      "Classification report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7737    0.7968    0.7851      7913\n",
      "           4     0.7952    0.7720    0.7834      8087\n",
      "\n",
      "    accuracy                         0.7843     16000\n",
      "   macro avg     0.7844    0.7844    0.7842     16000\n",
      "weighted avg     0.7846    0.7843    0.7842     16000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MNB unigram tf best model\n",
    "y_pred_mbn_2, vectorizer_mnb_2, clf_mnb_2 = model(X_train, y_train, X_test, y_test, CountVectorizer(encoding='latin-1', min_df=5, max_df=0.2), MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed:  9.7min\n",
      "[Parallel(n_jobs=-1)]: Done 144 out of 144 | elapsed: 12.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set:\n",
      "[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='latin-1',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=10, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True, vocabulary=None)), ('nb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]\n"
     ]
    }
   ],
   "source": [
    "# MNB unigram tf-idf tuning\n",
    "pipeline = Pipeline([('tfidf', TfidfVectorizer(encoding='latin-1')),('nb', MultinomialNB())])\n",
    "parameters = {\n",
    "    'tfidf__max_df': (0.01, 0.1, 0.2, 1.0),\n",
    "    'tfidf__min_df': (5, 10, 20, 1),\n",
    "    'nb__alpha': (1e-2, 1e-3, 1.0)\n",
    "}\n",
    "\n",
    "grid_search_tune = GridSearchCV(pipeline, parameters, cv=3, n_jobs=-1, verbose=3)\n",
    "grid_search_tune.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters set:\")\n",
    "print(grid_search_tune.best_estimator_.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_vec shape -  (1584000, 32041) \n",
      "\n",
      "X_test_vec shape -  (16000, 32041) \n",
      "\n",
      "Top  20  most and least informative features\n",
      "\t-14.6490\taarrghh        \t\t-5.3847\thave           \n",
      "\t-14.6490\tabscess        \t\t-5.3723\tso             \n",
      "\t-14.6490\talergies       \t\t-5.3721\tjust           \n",
      "\t-14.6490\talternator     \t\t-5.3545\tlove           \n",
      "\t-14.6490\talzheimer      \t\t-5.3093\twith           \n",
      "\t-14.6490\tantihistamines \t\t-5.2982\tthanks         \n",
      "\t-14.6490\tarrhh          \t\t-5.2898\tme             \n",
      "\t-14.6490\tauchh          \t\t-5.1913\ton             \n",
      "\t-14.6490\tayatollah      \t\t-5.1863\tof             \n",
      "\t-14.6490\tbackorder      \t\t-5.1199\tthat           \n",
      "\t-14.6490\tbandages       \t\t-5.1114\tin             \n",
      "\t-14.6490\tbarakatday     \t\t-5.0738\tgood           \n",
      "\t-14.6490\tbasij          \t\t-4.9708\tis             \n",
      "\t-14.6490\tbeefin         \t\t-4.9639\tmy             \n",
      "\t-14.6490\tblistered      \t\t-4.8838\tfor            \n",
      "\t-14.6490\tbodyguards     \t\t-4.8403\tand            \n",
      "\t-14.6490\tbogged         \t\t-4.7332\tit             \n",
      "\t-14.6490\tboohoohoo      \t\t-4.4578\tto             \n",
      "\t-14.6490\tbookieb        \t\t-4.4022\tthe            \n",
      "\t-14.6490\tboourns        \t\t-4.3356\tyou            \n",
      "\n",
      "Confusion matrix\n",
      " [[6213 1700]\n",
      " [1853 6234]] \n",
      "\n",
      "Classification report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7703    0.7852    0.7776      7913\n",
      "           4     0.7857    0.7709    0.7782      8087\n",
      "\n",
      "    accuracy                         0.7779     16000\n",
      "   macro avg     0.7780    0.7780    0.7779     16000\n",
      "weighted avg     0.7781    0.7779    0.7779     16000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MNB unigram tf-idf best model\n",
    "y_pred_mbn_3, vectorizer_mnb_3, clf_mnb_3 = model(X_train, y_train, X_test, y_test, TfidfVectorizer(encoding='latin-1', min_df=10), MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_vec shape -  (1584000, 3594521) \n",
      "\n",
      "X_test_vec shape -  (16000, 3594521) \n",
      "\n",
      "Top  20  most and least informative features\n",
      "\t-16.8362\taa after       \t\t-6.0337\tat             \n",
      "\t-16.8362\taa against     \t\t-5.9407\tbe             \n",
      "\t-16.8362\taa all         \t\t-5.7994\thave           \n",
      "\t-16.8362\taa allstars    \t\t-5.7967\tjust           \n",
      "\t-16.8362\taa already     \t\t-5.7891\tgood           \n",
      "\t-16.8362\taa another     \t\t-5.7612\twith           \n",
      "\t-16.8362\taa anyone      \t\t-5.7553\tso             \n",
      "\t-16.8362\taa are         \t\t-5.6682\tme             \n",
      "\t-16.8362\taa as          \t\t-5.5186\tthat           \n",
      "\t-16.8362\taa asyik       \t\t-5.5056\ton             \n",
      "\t-16.8362\taa at          \t\t-5.4268\tof             \n",
      "\t-16.8362\taa baas        \t\t-5.3219\tin             \n",
      "\t-16.8362\taa baby        \t\t-5.2479\tis             \n",
      "\t-16.8362\taa ball        \t\t-5.1736\tfor            \n",
      "\t-16.8362\taa batterys    \t\t-5.1028\tmy             \n",
      "\t-16.8362\taa bed         \t\t-4.9428\tit             \n",
      "\t-16.8362\taa beta        \t\t-4.9302\tand            \n",
      "\t-16.8362\taa boobie      \t\t-4.6496\tyou            \n",
      "\t-16.8362\taa boring      \t\t-4.4071\tto             \n",
      "\t-16.8362\taa bosen       \t\t-4.3551\tthe            \n",
      "\n",
      "Confusion matrix\n",
      " [[6651 1262]\n",
      " [1822 6265]] \n",
      "\n",
      "Classification report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7850    0.8405    0.8118      7913\n",
      "           4     0.8323    0.7747    0.8025      8087\n",
      "\n",
      "    accuracy                         0.8073     16000\n",
      "   macro avg     0.8087    0.8076    0.8071     16000\n",
      "weighted avg     0.8089    0.8073    0.8071     16000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MNB bigram baseline model\n",
    "y_pred_mbn_4, vectorizer_mnb_4, clf_mnb_4 = model(X_train, y_train, X_test, y_test, CountVectorizer(encoding='latin-1', ngram_range=(1,2)), MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 60 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed: 23.9min\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed: 34.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set:\n",
      "[('tf', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='latin-1',\n",
      "                input='content', lowercase=True, max_df=0.1, max_features=None,\n",
      "                min_df=2, ngram_range=(1, 2), preprocessor=None,\n",
      "                stop_words=None, strip_accents=None,\n",
      "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None,\n",
      "                vocabulary=None)), ('nb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]\n"
     ]
    }
   ],
   "source": [
    "# MNB bigram tf tuning\n",
    "pipeline = Pipeline([('tf', CountVectorizer(encoding='latin-1', ngram_range=(1,2))),('nb', MultinomialNB())])\n",
    "parameters = {\n",
    "    'tf__max_df': (0.05, 0.1, 0.2, 1.0),\n",
    "    'tf__min_df': (2, 5, 10, 20, 1),\n",
    "    'nb__alpha': (1e-2, 1e-3, 1.0)\n",
    "}\n",
    "\n",
    "grid_search_tune = GridSearchCV(pipeline, parameters, cv=3, n_jobs=-1, verbose=3)\n",
    "grid_search_tune.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters set:\")\n",
    "print(grid_search_tune.best_estimator_.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_vec shape -  (1584000, 1012914) \n",
      "\n",
      "X_test_vec shape -  (16000, 1012914) \n",
      "\n",
      "Top  20  most and least informative features\n",
      "\t-16.5273\taa at          \t\t-5.9050\tcan            \n",
      "\t-16.5273\taa boring      \t\t-5.9024\tthis           \n",
      "\t-16.5273\taa come        \t\t-5.9020\tup             \n",
      "\t-16.5273\taa demi        \t\t-5.8977\tall            \n",
      "\t-16.5273\taa did         \t\t-5.8432\tare            \n",
      "\t-16.5273\taa doesn       \t\t-5.8335\tyour           \n",
      "\t-16.5273\taa don         \t\t-5.8148\twas            \n",
      "\t-16.5273\taa feel        \t\t-5.7645\tlove           \n",
      "\t-16.5273\taa fuck        \t\t-5.7536\tday            \n",
      "\t-16.5273\taa hate        \t\t-5.7458\tbut            \n",
      "\t-16.5273\taa how         \t\t-5.7248\tat             \n",
      "\t-16.5273\taa it          \t\t-5.6318\tbe             \n",
      "\t-16.5273\taa its         \t\t-5.4904\thave           \n",
      "\t-16.5273\taa ive         \t\t-5.4878\tjust           \n",
      "\t-16.5273\taa man         \t\t-5.4801\tgood           \n",
      "\t-16.5273\taa on          \t\t-5.4523\twith           \n",
      "\t-16.5273\taa only        \t\t-5.4464\tso             \n",
      "\t-16.5273\taa poor        \t\t-5.3592\tme             \n",
      "\t-16.5273\taa really      \t\t-5.2096\tthat           \n",
      "\t-16.5273\taa sorry       \t\t-5.1967\ton             \n",
      "\n",
      "Confusion matrix\n",
      " [[6527 1386]\n",
      " [1675 6412]] \n",
      "\n",
      "Classification report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7958    0.8248    0.8101      7913\n",
      "           4     0.8223    0.7929    0.8073      8087\n",
      "\n",
      "    accuracy                         0.8087     16000\n",
      "   macro avg     0.8090    0.8089    0.8087     16000\n",
      "weighted avg     0.8092    0.8087    0.8087     16000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MNB bigram tf best model\n",
    "y_pred_mbn_5, vectorizer_mnb_5, clf_mnb_5 = model(X_train, y_train, X_test, y_test, CountVectorizer(encoding='latin-1', ngram_range=(1,2), min_df=2, max_df=0.1), MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 60 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed: 18.4min\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed: 29.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set:\n",
      "[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='latin-1',\n",
      "                input='content', lowercase=True, max_df=0.1, max_features=None,\n",
      "                min_df=2, ngram_range=(1, 2), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True, vocabulary=None)), ('nb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]\n"
     ]
    }
   ],
   "source": [
    "# MNB bigram tf-idf tuning\n",
    "pipeline = Pipeline([('tfidf', TfidfVectorizer(encoding='latin-1', ngram_range=(1,2))),('nb', MultinomialNB())])\n",
    "parameters = {\n",
    "    'tfidf__max_df': (0.05, 0.1, 0.2, 1.0),\n",
    "    'tfidf__min_df': (2, 5, 10, 20, 1),\n",
    "    'nb__alpha': (1e-2, 1e-3, 1.0)\n",
    "}\n",
    "\n",
    "grid_search_tune = GridSearchCV(pipeline, parameters, cv=3, n_jobs=-1, verbose=3)\n",
    "grid_search_tune.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters set:\")\n",
    "print(grid_search_tune.best_estimator_.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_vec shape -  (1584000, 1012914) \n",
      "\n",
      "X_test_vec shape -  (16000, 1012914) \n",
      "\n",
      "Top  20  most and least informative features\n",
      "\t-15.1928\taa at          \t\t-6.8585\tall            \n",
      "\t-15.1928\taa boring      \t\t-6.8525\tup             \n",
      "\t-15.1928\taa come        \t\t-6.8521\tnow            \n",
      "\t-15.1928\taa demi        \t\t-6.8196\twas            \n",
      "\t-15.1928\taa did         \t\t-6.8126\tlol            \n",
      "\t-15.1928\taa doesn       \t\t-6.7425\tat             \n",
      "\t-15.1928\taa don         \t\t-6.7192\tare            \n",
      "\t-15.1928\taa feel        \t\t-6.6887\tyour           \n",
      "\t-15.1928\taa fuck        \t\t-6.6698\tbe             \n",
      "\t-15.1928\taa hate        \t\t-6.6179\tday            \n",
      "\t-15.1928\taa how         \t\t-6.5515\thave           \n",
      "\t-15.1928\taa it          \t\t-6.5409\tso             \n",
      "\t-15.1928\taa its         \t\t-6.5297\tjust           \n",
      "\t-15.1928\taa ive         \t\t-6.4928\tlove           \n",
      "\t-15.1928\taa man         \t\t-6.4494\twith           \n",
      "\t-15.1928\taa on          \t\t-6.4467\tme             \n",
      "\t-15.1928\taa only        \t\t-6.3379\ton             \n",
      "\t-15.1928\taa poor        \t\t-6.2971\tthanks         \n",
      "\t-15.1928\taa really      \t\t-6.2907\tthat           \n",
      "\t-15.1928\taa sorry       \t\t-6.2086\tgood           \n",
      "\n",
      "Confusion matrix\n",
      " [[6561 1352]\n",
      " [1697 6390]] \n",
      "\n",
      "Classification report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7945    0.8291    0.8115      7913\n",
      "           4     0.8254    0.7902    0.8074      8087\n",
      "\n",
      "    accuracy                         0.8094     16000\n",
      "   macro avg     0.8099    0.8096    0.8094     16000\n",
      "weighted avg     0.8101    0.8094    0.8094     16000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MNB bigram tf-idf best model\n",
    "y_pred_mbn_6, vectorizer_mnb_6, clf_mnb_6 = model(X_train, y_train, X_test, y_test, TfidfVectorizer(encoding='latin-1', ngram_range=(1,2), min_df=2, max_df=0.1), MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printErrors(y_pred, top=10):\n",
    "    print(*([(X_test[i], y_test[i], y_pred[i]) for i in range(len(y_test)) if y_pred[i] != y_test[i]][:top]), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram tf model\n",
      "('where the are my pinking shears rarararrarararr babyproofing while cutting stuff makes me stick shears random places forget them', 0, 4)\n",
      "('not bad bit grumpy cause of exams but generally ok ta', 4, 0)\n",
      "('can watch it what is it', 0, 4)\n",
      "('song of my life now your love is lie simple plan beautifulylost', 0, 4)\n",
      "('watching the last leno so glad got to go once', 0, 4)\n",
      "('dropped your books off in the library', 4, 0)\n",
      "('sun burns are noo fun bored sittin at home watching bride wars with my sister have good weekend everyone', 0, 4)\n",
      "('looking for mascot for needs to be an octopus but the one youhave isnt cutting it if sign up ull see', 0, 4)\n",
      "('no my dear then you just do what am doing read it again', 4, 0)\n",
      "('umm its getting betterr than before but its still pretty bad lol', 4, 0)\n",
      "('huh turns out like marmite when did stop being loved by all', 0, 4)\n",
      "('arrgghh going to sydney for the first time on the holidays and then off to canberra and somewhere else in snow', 4, 0)\n",
      "('thanks really appreciate that my skin is acting up', 0, 4)\n",
      "('just ordered ufc undisputed bring on the octagon pain', 4, 0)\n",
      "('can feel my bottom and is now going to bed', 4, 0)\n",
      "('going to watch drag me to hell at the bridgeat oo scary movie', 4, 0)\n",
      "('off to get ready for school now', 4, 0)\n",
      "('still trying to upload pics of my new hair gave myself first aide cut my hand with knife chopping onions ugh', 4, 0)\n",
      "('you re absolutely right was thinking today was the th oh well next year you must tell me all about it next week at', 0, 4)\n",
      "('ads on fb are just getting mean', 0, 4)\n",
      "\n",
      "\n",
      "Bigram tf model\n",
      "('where the are my pinking shears rarararrarararr babyproofing while cutting stuff makes me stick shears random places forget them', 0, 4)\n",
      "('reply me pls', 4, 0)\n",
      "('not bad bit grumpy cause of exams but generally ok ta', 4, 0)\n",
      "('song of my life now your love is lie simple plan beautifulylost', 0, 4)\n",
      "('watching the last leno so glad got to go once', 0, 4)\n",
      "('sun burns are noo fun bored sittin at home watching bride wars with my sister have good weekend everyone', 0, 4)\n",
      "('umm its getting betterr than before but its still pretty bad lol', 4, 0)\n",
      "('huh turns out like marmite when did stop being loved by all', 0, 4)\n",
      "('arrgghh going to sydney for the first time on the holidays and then off to canberra and somewhere else in snow', 4, 0)\n",
      "('thanks really appreciate that my skin is acting up', 0, 4)\n",
      "('can feel my bottom and is now going to bed', 4, 0)\n",
      "('my leg is tired so tired but it paid off when saw his smile wide as ever sweeter than sugar surprise that is for you ily', 4, 0)\n",
      "('off to get ready for school now', 4, 0)\n",
      "('still trying to upload pics of my new hair gave myself first aide cut my hand with knife chopping onions ugh', 4, 0)\n",
      "('you re absolutely right was thinking today was the th oh well next year you must tell me all about it next week at', 0, 4)\n",
      "('ads on fb are just getting mean', 0, 4)\n",
      "('extremely tired and can wait to see erick', 4, 0)\n",
      "('wish was too', 4, 0)\n",
      "('konstantino firefox crash last months both in mac and pc and is very sloow', 4, 0)\n",
      "('oh damn missed it sorry glad it was good though', 4, 0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Unigram tf model\")\n",
    "printErrors(y_pred_mbn_1, top=20)\n",
    "print()\n",
    "# print(\"Unigram tf-idf model\")\n",
    "# printErrors(y_pred_mbn_3, top=20)\n",
    "print()\n",
    "print(\"Bigram tf model\")\n",
    "printErrors(y_pred_mbn_5, top=20)\n",
    "print()\n",
    "# print(\"Bigram tf-idf model\")\n",
    "# printErrors(y_pred_mbn_6, top=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
