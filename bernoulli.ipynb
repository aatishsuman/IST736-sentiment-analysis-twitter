{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as p\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count    Dtype \n",
      "---  ------      --------------    ----- \n",
      " 0   target      1600000 non-null  int64 \n",
      " 1   ids         1600000 non-null  int64 \n",
      " 2   date        1600000 non-null  object\n",
      " 3   flag        1600000 non-null  object\n",
      " 4   user        1600000 non-null  object\n",
      " 5   text        1600000 non-null  object\n",
      " 6   clean_text  1600000 non-null  object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 85.4+ MB\n"
     ]
    }
   ],
   "source": [
    "%store -r data\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600000\n",
      "1600000\n"
     ]
    }
   ],
   "source": [
    "X = data['clean_text'].values\n",
    "y = data['target'].values\n",
    "print(len(X))\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1584000,) (1584000,) (16000,) (16000,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.01, random_state=0)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_most_and_least_informative_features(vectorizer, clf, n=10):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[-n:])\n",
    "    print(\"Top \", n, \" most and least informative features\")\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print(\"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, y_train, X_test, y_test, vectorizer, clf):\n",
    "    \n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    print(\"X_train_vec shape - \", X_train_vec.shape, \"\\n\")\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    print(\"X_test_vec shape - \", X_test_vec.shape, \"\\n\")\n",
    "    \n",
    "    y_pred = clf.fit(X_train_vec, y_train).predict(X_test_vec)\n",
    "    show_most_and_least_informative_features(vectorizer, clf, n=20)\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=[0,4])\n",
    "    print()\n",
    "    print(\"Confusion matrix\\n\", cm, \"\\n\")\n",
    "\n",
    "    print(\"Classification report\\n\", classification_report(y_test, y_pred, target_names=['0','4'], digits=4))\n",
    "    \n",
    "    return y_pred, vectorizer, clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_vec shape -  (1584000, 243575) \n",
      "\n",
      "X_test_vec shape -  (16000, 243575) \n",
      "\n",
      "Top  20  most and least informative features\n",
      "\t-13.5822\taabot          \t\t-2.8219\tbut            \n",
      "\t-13.5822\taabout         \t\t-2.7345\tbe             \n",
      "\t-13.5822\taabt           \t\t-2.5922\thave           \n",
      "\t-13.5822\taacattyisamazing\t\t-2.5831\tgood           \n",
      "\t-13.5822\taacchhoo       \t\t-2.5678\tjust           \n",
      "\t-13.5822\taacck          \t\t-2.5637\tso             \n",
      "\t-13.5822\taacckk         \t\t-2.5438\twith           \n",
      "\t-13.5822\taach           \t\t-2.4756\tme             \n",
      "\t-13.5822\taachar         \t\t-2.3313\tthat           \n",
      "\t-13.5822\taachee         \t\t-2.3106\ton             \n",
      "\t-13.5822\taachhoo        \t\t-2.2492\tof             \n",
      "\t-13.5822\taacount        \t\t-2.1482\tin             \n",
      "\t-13.5822\taacs           \t\t-2.0675\tis             \n",
      "\t-13.5822\taadaamm        \t\t-1.9901\tfor            \n",
      "\t-13.5822\taadam          \t\t-1.9487\tmy             \n",
      "\t-13.5822\taaden          \t\t-1.8386\tit             \n",
      "\t-13.5822\taadha          \t\t-1.7848\tand            \n",
      "\t-13.5822\taadn           \t\t-1.6096\tyou            \n",
      "\t-13.5822\taaeeaa         \t\t-1.3382\tto             \n",
      "\t-13.5822\taaeew          \t\t-1.3004\tthe            \n",
      "\n",
      "Confusion matrix\n",
      " [[6216 1697]\n",
      " [1717 6370]] \n",
      "\n",
      "Classification report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7836    0.7855    0.7846      7913\n",
      "           4     0.7896    0.7877    0.7887      8087\n",
      "\n",
      "    accuracy                         0.7866     16000\n",
      "   macro avg     0.7866    0.7866    0.7866     16000\n",
      "weighted avg     0.7866    0.7866    0.7866     16000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Bernoulli NB unigram baseline model\n",
    "y_pred_mbn_1, vectorizer_mnb_1, clf_mnb_1 = model(X_train, y_train, X_test, y_test, CountVectorizer(encoding='latin-1', binary=True), BernoulliNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed: 12.5min\n",
      "[Parallel(n_jobs=-1)]: Done 144 out of 144 | elapsed: 20.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set:\n",
      "[('tf', CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='latin-1',\n",
      "                input='content', lowercase=True, max_df=0.2, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 1), preprocessor=None,\n",
      "                stop_words=None, strip_accents=None,\n",
      "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None,\n",
      "                vocabulary=None)), ('nb', BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True))]\n"
     ]
    }
   ],
   "source": [
    "# Bernoulli NB unigram tf tuning\n",
    "pipeline = Pipeline([('tf', CountVectorizer(encoding='latin-1', binary=True)),('nb', BernoulliNB())])\n",
    "parameters = {\n",
    "    'tf__max_df': (0.01, 0.1, 0.2, 1.0),\n",
    "    'tf__min_df': (5, 10, 20, 1),\n",
    "    'nb__alpha': (1e-2, 1e-3, 1.0)\n",
    "}\n",
    "\n",
    "grid_search_tune = GridSearchCV(pipeline, parameters, cv=3, n_jobs=-1, verbose=3)\n",
    "grid_search_tune.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters set:\")\n",
    "print(grid_search_tune.best_estimator_.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_vec shape -  (1584000, 243573) \n",
      "\n",
      "X_test_vec shape -  (16000, 243573) \n",
      "\n",
      "Top  20  most and least informative features\n",
      "\t-13.5822\taabot          \t\t-2.8733\tday            \n",
      "\t-13.5822\taabout         \t\t-2.8272\tat             \n",
      "\t-13.5822\taabt           \t\t-2.8219\tbut            \n",
      "\t-13.5822\taacattyisamazing\t\t-2.7345\tbe             \n",
      "\t-13.5822\taacchhoo       \t\t-2.5922\thave           \n",
      "\t-13.5822\taacck          \t\t-2.5831\tgood           \n",
      "\t-13.5822\taacckk         \t\t-2.5678\tjust           \n",
      "\t-13.5822\taach           \t\t-2.5637\tso             \n",
      "\t-13.5822\taachar         \t\t-2.5438\twith           \n",
      "\t-13.5822\taachee         \t\t-2.4756\tme             \n",
      "\t-13.5822\taachhoo        \t\t-2.3313\tthat           \n",
      "\t-13.5822\taacount        \t\t-2.3106\ton             \n",
      "\t-13.5822\taacs           \t\t-2.2492\tof             \n",
      "\t-13.5822\taadaamm        \t\t-2.1482\tin             \n",
      "\t-13.5822\taadam          \t\t-2.0675\tis             \n",
      "\t-13.5822\taaden          \t\t-1.9901\tfor            \n",
      "\t-13.5822\taadha          \t\t-1.9487\tmy             \n",
      "\t-13.5822\taadn           \t\t-1.8386\tit             \n",
      "\t-13.5822\taaeeaa         \t\t-1.7848\tand            \n",
      "\t-13.5822\taaeew          \t\t-1.6096\tyou            \n",
      "\n",
      "Confusion matrix\n",
      " [[6196 1717]\n",
      " [1726 6361]] \n",
      "\n",
      "Classification report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7821    0.7830    0.7826      7913\n",
      "           4     0.7874    0.7866    0.7870      8087\n",
      "\n",
      "    accuracy                         0.7848     16000\n",
      "   macro avg     0.7848    0.7848    0.7848     16000\n",
      "weighted avg     0.7848    0.7848    0.7848     16000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MNB unigram tf best model\n",
    "y_pred_mbn_2, vectorizer_mnb_2, clf_mnb_2 = model(X_train, y_train, X_test, y_test, CountVectorizer(encoding='latin-1', binary=True, max_df=0.2), BernoulliNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_vec shape -  (1584000, 3594521) \n",
      "\n",
      "X_test_vec shape -  (16000, 3594521) \n",
      "\n",
      "Top  20  most and least informative features\n",
      "\t-13.5822\taa after       \t\t-2.8219\tbut            \n",
      "\t-13.5822\taa against     \t\t-2.7345\tbe             \n",
      "\t-13.5822\taa all         \t\t-2.5922\thave           \n",
      "\t-13.5822\taa allstars    \t\t-2.5831\tgood           \n",
      "\t-13.5822\taa already     \t\t-2.5678\tjust           \n",
      "\t-13.5822\taa another     \t\t-2.5637\tso             \n",
      "\t-13.5822\taa anyone      \t\t-2.5438\twith           \n",
      "\t-13.5822\taa are         \t\t-2.4756\tme             \n",
      "\t-13.5822\taa as          \t\t-2.3313\tthat           \n",
      "\t-13.5822\taa asyik       \t\t-2.3106\ton             \n",
      "\t-13.5822\taa at          \t\t-2.2492\tof             \n",
      "\t-13.5822\taa baas        \t\t-2.1482\tin             \n",
      "\t-13.5822\taa baby        \t\t-2.0675\tis             \n",
      "\t-13.5822\taa ball        \t\t-1.9901\tfor            \n",
      "\t-13.5822\taa batterys    \t\t-1.9487\tmy             \n",
      "\t-13.5822\taa bed         \t\t-1.8386\tit             \n",
      "\t-13.5822\taa beta        \t\t-1.7848\tand            \n",
      "\t-13.5822\taa boobie      \t\t-1.6096\tyou            \n",
      "\t-13.5822\taa boring      \t\t-1.3382\tto             \n",
      "\t-13.5822\taa bosen       \t\t-1.3004\tthe            \n",
      "\n",
      "Confusion matrix\n",
      " [[6317 1596]\n",
      " [1526 6561]] \n",
      "\n",
      "Classification report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8054    0.7983    0.8019      7913\n",
      "           4     0.8043    0.8113    0.8078      8087\n",
      "\n",
      "    accuracy                         0.8049     16000\n",
      "   macro avg     0.8049    0.8048    0.8048     16000\n",
      "weighted avg     0.8049    0.8049    0.8049     16000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Bernoulli NB bigram baseline model\n",
    "y_pred_mbn_3, vectorizer_mnb_3, clf_mnb_3 = model(X_train, y_train, X_test, y_test, CountVectorizer(encoding='latin-1', binary=True, ngram_range=(1,2)), BernoulliNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done  58 out of  60 | elapsed: 19.9min remaining:   41.0s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 21.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set:\n",
      "[('tf', CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='latin-1',\n",
      "                input='content', lowercase=True, max_df=0.05, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 2), preprocessor=None,\n",
      "                stop_words=None, strip_accents=None,\n",
      "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None,\n",
      "                vocabulary=None)), ('nb', BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True))]\n"
     ]
    }
   ],
   "source": [
    "# Bernoulli NB bigram tf tuning\n",
    "pipeline = Pipeline([('tf', CountVectorizer(encoding='latin-1', binary=True, ngram_range=(1,2))),('nb', BernoulliNB())])\n",
    "parameters = {\n",
    "    'tf__max_df': (0.05, 0.1, 0.2, 1.0),\n",
    "    'tf__min_df': (2, 5, 10, 20, 1)\n",
    "}\n",
    "\n",
    "grid_search_tune = GridSearchCV(pipeline, parameters, cv=3, n_jobs=-1, verbose=3)\n",
    "grid_search_tune.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters set:\")\n",
    "print(grid_search_tune.best_estimator_.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_vec shape -  (1584000, 3594492) \n",
      "\n",
      "X_test_vec shape -  (16000, 3594492) \n",
      "\n",
      "Top  20  most and least informative features\n",
      "\t-13.5822\taa after       \t\t-3.4327\tnew            \n",
      "\t-13.5822\taa against     \t\t-3.3809\tgo             \n",
      "\t-13.5822\taa all         \t\t-3.3807\tgot            \n",
      "\t-13.5822\taa allstars    \t\t-3.3736\tfrom           \n",
      "\t-13.5822\taa already     \t\t-3.3476\ttoo            \n",
      "\t-13.5822\taa another     \t\t-3.3424\twill           \n",
      "\t-13.5822\taa anyone      \t\t-3.3121\twhat           \n",
      "\t-13.5822\taa are         \t\t-3.3044\ttime           \n",
      "\t-13.5822\taa as          \t\t-3.3005\ttoday          \n",
      "\t-13.5822\taa asyik       \t\t-3.2912\tdo             \n",
      "\t-13.5822\taa at          \t\t-3.2813\tgoing          \n",
      "\t-13.5822\taa baas        \t\t-3.2227\twe             \n",
      "\t-13.5822\taa baby        \t\t-3.1592\tthanks         \n",
      "\t-13.5822\taa ball        \t\t-3.1462\tlol            \n",
      "\t-13.5822\taa batterys    \t\t-3.1202\tget            \n",
      "\t-13.5822\taa bed         \t\t-3.1016\tlike           \n",
      "\t-13.5822\taa beta        \t\t-3.0656\tout            \n",
      "\t-13.5822\taa boobie      \t\t-2.9604\tyour           \n",
      "\t-13.5822\taa boring      \t\t-2.9420\tare            \n",
      "\t-13.5822\taa bosen       \t\t-2.8745\tlove           \n",
      "\n",
      "Confusion matrix\n",
      " [[6331 1582]\n",
      " [1496 6591]] \n",
      "\n",
      "Classification report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8089    0.8001    0.8044      7913\n",
      "           4     0.8064    0.8150    0.8107      8087\n",
      "\n",
      "    accuracy                         0.8076     16000\n",
      "   macro avg     0.8077    0.8075    0.8076     16000\n",
      "weighted avg     0.8076    0.8076    0.8076     16000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Bernoulli NB bigram tf best model\n",
    "y_pred_mbn_4, vectorizer_mnb_4, clf_mnb_4 = model(X_train, y_train, X_test, y_test, CountVectorizer(encoding='latin-1', binary=True, ngram_range=(1,2), max_df=0.05), BernoulliNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printErrors(y_pred, top=10):\n",
    "    print(*([(X_test[i], y_test[i], y_pred[i]) for i in range(len(y_test)) if y_pred[i] != y_test[i]][:top]), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram tf model\n",
      "('where the are my pinking shears rarararrarararr babyproofing while cutting stuff makes me stick shears random places forget them', 0, 4)\n",
      "('not bad bit grumpy cause of exams but generally ok ta', 4, 0)\n",
      "('can watch it what is it', 0, 4)\n",
      "('song of my life now your love is lie simple plan beautifulylost', 0, 4)\n",
      "('watching the last leno so glad got to go once', 0, 4)\n",
      "('dropped your books off in the library', 4, 0)\n",
      "('do more that anything', 0, 4)\n",
      "('looking for mascot for needs to be an octopus but the one youhave isnt cutting it if sign up ull see', 0, 4)\n",
      "('once again show is rescheduled', 0, 4)\n",
      "('no my dear then you just do what am doing read it again', 4, 0)\n",
      "('umm its getting betterr than before but its still pretty bad lol', 4, 0)\n",
      "('huh turns out like marmite when did stop being loved by all', 0, 4)\n",
      "('arrgghh going to sydney for the first time on the holidays and then off to canberra and somewhere else in snow', 4, 0)\n",
      "('thanks really appreciate that my skin is acting up', 0, 4)\n",
      "('just ordered ufc undisputed bring on the octagon pain', 4, 0)\n",
      "('can feel my bottom and is now going to bed', 4, 0)\n",
      "('off to get ready for school now', 4, 0)\n",
      "('still trying to upload pics of my new hair gave myself first aide cut my hand with knife chopping onions ugh', 4, 0)\n",
      "('you re absolutely right was thinking today was the th oh well next year you must tell me all about it next week at', 0, 4)\n",
      "('ads on fb are just getting mean', 0, 4)\n",
      "\n",
      "\n",
      "Bigram tf model\n",
      "('where the are my pinking shears rarararrarararr babyproofing while cutting stuff makes me stick shears random places forget them', 0, 4)\n",
      "('not bad bit grumpy cause of exams but generally ok ta', 4, 0)\n",
      "('can watch it what is it', 0, 4)\n",
      "('song of my life now your love is lie simple plan beautifulylost', 0, 4)\n",
      "('aah tired haven chilled minute today', 0, 4)\n",
      "('watching the last leno so glad got to go once', 0, 4)\n",
      "('sun burns are noo fun bored sittin at home watching bride wars with my sister have good weekend everyone', 0, 4)\n",
      "('do more that anything', 0, 4)\n",
      "('looking for mascot for needs to be an octopus but the one youhave isnt cutting it if sign up ull see', 0, 4)\n",
      "('umm its getting betterr than before but its still pretty bad lol', 4, 0)\n",
      "('huh turns out like marmite when did stop being loved by all', 0, 4)\n",
      "('arrgghh going to sydney for the first time on the holidays and then off to canberra and somewhere else in snow', 4, 0)\n",
      "('thanks really appreciate that my skin is acting up', 0, 4)\n",
      "('can feel my bottom and is now going to bed', 4, 0)\n",
      "('my leg is tired so tired but it paid off when saw his smile wide as ever sweeter than sugar surprise that is for you ily', 4, 0)\n",
      "('off to get ready for school now', 4, 0)\n",
      "('still trying to upload pics of my new hair gave myself first aide cut my hand with knife chopping onions ugh', 4, 0)\n",
      "('you re absolutely right was thinking today was the th oh well next year you must tell me all about it next week at', 0, 4)\n",
      "('ads on fb are just getting mean', 0, 4)\n",
      "('wish was too', 4, 0)\n"
     ]
    }
   ],
   "source": [
    "print(\"Unigram tf model\")\n",
    "printErrors(y_pred_mbn_1, top=20)\n",
    "print()\n",
    "print()\n",
    "print(\"Bigram tf model\")\n",
    "printErrors(y_pred_mbn_4, top=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
